import os
import json
from collections import Counter, defaultdict
from datetime import datetime
from zoneinfo import ZoneInfo
from pathlib import Path
import time
import shutil
import subprocess
import traceback

import joblib
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
import mlflow
import mlflow.sklearn

# MLflow ì„¤ì •
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# ===============================
# 0) ê²½ë¡œ/í™˜ê²½ + SSH ìœ í‹¸ (ì˜êµ¬ í•´ê²°)
# ===============================
BASE_DIR = Path(__file__).resolve().parent  # /app

# ğŸ¯ ì²´ê³„ì ì¸ ëª¨ë¸ ì €ì¥ êµ¬ì¡° (í™˜ê²½ë³€ìˆ˜ë¡œ ë°›ì•„ì„œ ìœ ì—°í•˜ê²Œ ì„¤ì •)
GUEST_MODELS_DIR = Path(os.getenv("OUTPUT_DIR", "/opt/airflow/models/guest/latest")).parent.parent
GUEST_LATEST_DIR = Path(os.getenv("OUTPUT_DIR", "/opt/airflow/models/guest/latest"))
GUEST_ARCHIVE_DIR = GUEST_MODELS_DIR / "archive"
GUEST_SERVING_DIR = GUEST_MODELS_DIR / "serving"

# ë””ë ‰í† ë¦¬ ìƒì„±
for dir_path in [GUEST_LATEST_DIR, GUEST_ARCHIVE_DIR, GUEST_SERVING_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# í˜„ì¬ í•™ìŠµìš© ë””ë ‰í† ë¦¬ (latest ì‚¬ìš©)
GUEST_EXPORT_DIR = GUEST_LATEST_DIR

# DB ì—°ê²° ì„¤ì • (Airflow DB ì‚¬ìš©)
DB_HOST = os.getenv("DB_HOST", "sobi-db")
DB_NAME = "airflowdb"
DB_USER = "airflow"
DB_PASSWORD = "airflow123"
DB_PORT = int(os.getenv("DB_PORT", "5432"))

def p(x) -> str:
    return str(x)

SSH_OPTS = (
    "-o BatchMode=yes -o PreferredAuthentications=publickey -o IdentitiesOnly=yes "
    "-o StrictHostKeyChecking=no -o ConnectTimeout=5"
)

def ensure_ssh_key() -> str:
    """RO ë§ˆìš´íŠ¸ëœ í‚¤ë¥¼ /tmpë¡œ ë³µì‚¬í•˜ê³  ê¶Œí•œ 600ìœ¼ë¡œ ë§ì¶° ë°˜í™˜"""
    src = "/mnt/ssh/id_ed25519_jetson"
    if not os.path.exists(src):
        raise RuntimeError(f"SSH key not found: {src}")
    tmp_dir = "/tmp/ssh"
    os.makedirs(tmp_dir, exist_ok=True)
    dst = os.path.join(tmp_dir, "jetson_key")
    shutil.copy2(src, dst)
    os.chmod(dst, 0o600)
    return dst

def ssh_test():
    key = ensure_ssh_key()
    host = os.getenv("JETSON_HOST", "host.docker.internal")
    port = os.getenv("JETSON_SSH_PORT", "2222")
    user = os.getenv("JETSON_USER", "ssafy")
    cmd = f'ssh -i {key} -p {port} {SSH_OPTS} {user}@{host} "true"'
    subprocess.run(cmd, shell=True, check=True)

def rsync_push(src_dir: Path, dest_dir_env: str):
    key = ensure_ssh_key()
    host = os.getenv("JETSON_HOST", "host.docker.internal")
    port = os.getenv("JETSON_SSH_PORT", "2222")
    user = os.getenv("JETSON_USER", "ssafy")
    dest = os.getenv(dest_dir_env)
    if not dest:
        raise RuntimeError(f"{dest_dir_env} env required")
    src = p(src_dir).rstrip("/") + "/"
    dest = dest.rstrip("/") + "/"
    cmd = (
        f'rsync -azv --delete --partial --inplace --timeout=300 '
        f'-e "ssh -i {key} -p {port} {SSH_OPTS} -o ServerAliveInterval=30 -o ServerAliveCountMax=3 -o TCPKeepAlive=yes" '
        f'{src} {user}@{host}:{dest}'
    )
    subprocess.run(cmd, shell=True, check=True)

# ===============================
# 1) ë¡œë”/ì „ì²˜ë¦¬
# ===============================
def get_kst_today() -> str:
    return datetime.now(ZoneInfo("Asia/Seoul")).date().isoformat()

def load_guest_data_from_airflow_db() -> pd.DataFrame:
    """Airflow DBì—ì„œ ê²ŒìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ë¡œë“œ (ìµœê·¼ ì¼ì£¼ì¼)"""
    try:
        # Airflow DB ì—°ê²°
        db_url = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
        engine = create_engine(db_url)
        
        # ìµœê·¼ ì¼ì£¼ì¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        query = """
        SELECT user_id, product_id, product_name, category, price, purchase_date, age, gender
        FROM airflow_data.training_data
        WHERE purchase_date >= CURRENT_DATE - INTERVAL '7 days'
        ORDER BY purchase_date DESC
        """
        
        df = pd.read_sql(query, engine)
        engine.dispose()
        
        if df.empty:
            print("[WARN] Airflow DBì—ì„œ ìµœê·¼ ì¼ì£¼ì¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
            
        print(f"[INFO] Airflow DBì—ì„œ ìµœê·¼ ì¼ì£¼ì¼ ë°ì´í„° {len(df)}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        return df
        
    except Exception as e:
        print(f"[ERROR] Airflow DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def load_dataframe() -> pd.DataFrame:
    """ë°ì´í„° ë¡œë“œ ë©”ì¸ í•¨ìˆ˜"""
    # ë¨¼ì € Airflow DBì—ì„œ ì‹œë„
    df = load_guest_data_from_airflow_db()
    
    if df.empty:
        print("[WARN] Airflow DBì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ CSV íŒŒì¼ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        # CSV íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„ (ê¸°ì¡´ ë¡œì§)
        csv_path = Path(os.getenv("CSV_PATH", BASE_DIR / "data" / "purchased_history.csv"))
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            if "purchase_date" in df.columns:
                df["purchase_date"] = pd.to_datetime(df["purchase_date"])
                latest_date = df["purchase_date"].max().date()
                df = df[df["purchase_date"].dt.date == latest_date]
                print(f"[INFO] CSVì—ì„œ {latest_date} ë‚ ì§œ ë°ì´í„° {len(df)}ê°œ í–‰ ë¡œë“œ")
            else:
                print("[WARN] CSVì— purchase_date ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                df = pd.DataFrame()
        else:
            print(f"[WARN] CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            df = pd.DataFrame()
    
    return df

def flatten_receipts(df_receipt: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for _, row in df_receipt.iterrows():
        session_id = str(row.get("purchased_at"))
        user_id = row.get("user_id")
        
        if pd.isna(row.get("product_list")) or row.get("product_list") == "":
            continue
            
        products = str(row.get("product_list")).split(",")
        for product in products:
            product = product.strip()
            if product:
                rows.append({
                    "session_id": session_id,
                    "user_id": user_id,
                    "product_id": product
                })
    
    return pd.DataFrame(rows)

def create_user_product_matrix(df: pd.DataFrame) -> pd.DataFrame:
    """ì‚¬ìš©ì-ìƒí’ˆ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
    user_product = df.groupby(['user_id', 'product_id']).size().reset_index(name='purchase_count')
    matrix = user_product.pivot(index='user_id', columns='product_id', values='purchase_count').fillna(0)
    return matrix

def create_product_features(df: pd.DataFrame) -> pd.DataFrame:
    """ìƒí’ˆë³„ íŠ¹ì„± ìƒì„± (ì»¬ëŸ¼ ìœ ë¬´ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ)"""
    agg = {'price': ['mean', 'std', 'count']}

    # categoryê°€ ìˆìœ¼ë©´ ìµœë¹ˆê°’
    if 'category' in df.columns:
        agg['category'] = (lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown')

    # genderê°€ ìˆìœ¼ë©´ ìµœë¹ˆê°’
    if 'gender' in df.columns:
        agg['gender'] = (lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown')

    # ageê°€ ìˆìœ¼ë©´ í‰ê· /í‘œì¤€í¸ì°¨
    if 'age' in df.columns:
        agg['age'] = ['mean', 'std']

    product_features = df.groupby('product_id').agg(agg).reset_index()

    # ì»¬ëŸ¼ëª… í‰íƒ„í™”
    product_features.columns = [
        '_'.join(col).strip('_') if isinstance(col, tuple) else col
        for col in product_features.columns
    ]

    # ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì±„ìš°ê¸°
    if 'category_<lambda>' not in product_features.columns:
        product_features['category_<lambda>'] = 'unknown'
    if 'gender_<lambda>' not in product_features.columns:
        product_features['gender_<lambda>'] = 'unknown'
    if 'age_mean' not in product_features.columns:
        product_features['age_mean'] = np.nan
    if 'age_std' not in product_features.columns:
        product_features['age_std'] = np.nan

    # ìµœì¢… ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
    rename_map = {
        'price_mean': 'avg_price',
        'price_std': 'std_price',
        'price_count': 'purchase_count',
        'category_<lambda>': 'main_category',
        'gender_<lambda>': 'main_gender',
        'age_mean': 'avg_age',
        'age_std': 'std_age',
    }
    product_features = product_features.rename(columns=rename_map)

    return product_features
    
    return product_features

# ===============================
# 2) ëª¨ë¸ í•™ìŠµ
# ===============================
def train_guest_model(df_training: pd.DataFrame, experiment_name: str = "guest_recommendation"):
    """ê²ŒìŠ¤íŠ¸ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ (MLflow í†µí•©)"""
    
    # MLflow ì‹¤í—˜ ì„¤ì •
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run(run_name=f"guest_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            "n_neighbors": 10,
            "metric": "cosine",
            "algorithm": "auto",
            "leaf_size": 30
        }
        
        # MLflowì— íŒŒë¼ë¯¸í„° ê¸°ë¡
        mlflow.log_params(params)
        
        print("=== ê²ŒìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
        start_time = time.time()
        
        # ì‚¬ìš©ì-ìƒí’ˆ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        user_product_matrix = create_user_product_matrix(df_training)
        print(f"ì‚¬ìš©ì-ìƒí’ˆ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ: {user_product_matrix.shape}")
        
        # ìƒí’ˆë³„ íŠ¹ì„± ìƒì„±
        product_features = create_product_features(df_training)
        print(f"ìƒí’ˆë³„ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {product_features.shape}")
        
        # ğŸ¯ ê¸°ì¡´ ëª¨ë¸ í™•ì¸ ë° í•™ìŠµ ëª¨ë“œ ì „í™˜
        existing_model_path = GUEST_LATEST_DIR / "guest_model.pkl"
        existing_tfidf_path = GUEST_LATEST_DIR / "tfidf_vectorizer.pkl"
        
        if existing_model_path.exists() and existing_tfidf_path.exists():
            print("ğŸ”„ ê¸°ì¡´ ê²ŒìŠ¤íŠ¸ ëª¨ë¸ ë°œê²¬!")
            print("ğŸ“š ê¸°ì¡´ íŠ¹ì„± ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ íš¨ìœ¨ì  í•™ìŠµ...")
            
            try:
                # ê¸°ì¡´ ëª¨ë¸ê³¼ TF-IDF ë¡œë“œ
                existing_model = joblib.load(existing_model_path)
                existing_tfidf = joblib.load(existing_tfidf_path)
                
                # ê¸°ì¡´ íŠ¹ì„±ê³¼ ìƒˆë¡œìš´ íŠ¹ì„± ë¹„êµ
                existing_features = set(existing_tfidf.get_feature_names_out())
                new_features = set(product_features['main_category'].fillna('unknown').astype(str))
                
                print(f"   - ê¸°ì¡´ íŠ¹ì„± ìˆ˜: {len(existing_features)}")
                print(f"   - ìƒˆë¡œìš´ íŠ¹ì„± ìˆ˜: {len(new_features)}")
                print(f"   - ê³µí†µ íŠ¹ì„± ìˆ˜: {len(existing_features & new_features)}")
                
                # íŠ¹ì„± ë³€í™”ê°€ í¬ë©´ ìƒˆë¡œ í•™ìŠµ, ì‘ìœ¼ë©´ ê¸°ì¡´ ëª¨ë¸ í™œìš©
                feature_change_ratio = len(new_features - existing_features) / len(existing_features)
                
                if feature_change_ratio > 0.3:  # 30% ì´ìƒ ë³€í™” ì‹œ
                    print("âš ï¸ íŠ¹ì„± ë³€í™”ê°€ í½ë‹ˆë‹¤. ì „ì²´ ì¬í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                    raise Exception("íŠ¹ì„± ë³€í™”ë¡œ ì¸í•œ ì¬í•™ìŠµ í•„ìš”")
                else:
                    print("âœ… íŠ¹ì„± ë³€í™”ê°€ ì ìŠµë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                    # ê¸°ì¡´ ëª¨ë¸ êµ¬ì¡° ìœ ì§€, ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
                    model = existing_model
                    tfidf = existing_tfidf
                    
                    # ìƒˆë¡œìš´ íŠ¹ì„±ìœ¼ë¡œ ë²¡í„°í™” (ê¸°ì¡´ ì–´íœ˜ ìœ ì§€)
                    product_text = product_features['main_category'].fillna('unknown').astype(str)
                    product_vectors = tfidf.transform(product_text)  # fit_transform ëŒ€ì‹  transform ì‚¬ìš©
                    
                    # ëª¨ë¸ ì¬í•™ìŠµ
                    model.fit(product_vectors)
                    print("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ í™œìš© ì‹¤íŒ¨, ìƒˆë¡œ í•™ìŠµ: {e}")
                # ìƒˆë¡œ í•™ìŠµ
                model = NearestNeighbors(
                    n_neighbors=params["n_neighbors"],
                    metric=params["metric"],
                    algorithm=params["algorithm"],
                    leaf_size=params["leaf_size"]
                )
                
                # ìƒí’ˆ íŠ¹ì„±ì„ TF-IDFë¡œ ë²¡í„°í™”
                tfidf = TfidfVectorizer(max_features=1000, stop_words=None)
                product_text = product_features['main_category'].fillna('unknown').astype(str)
                product_vectors = tfidf.fit_transform(product_text)
                
                # ëª¨ë¸ í•™ìŠµ
                model.fit(product_vectors)
                print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        else:
            print("ğŸ†• ìƒˆë¡œìš´ ê²ŒìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            # ìƒˆë¡œ í•™ìŠµ
            model = NearestNeighbors(
                n_neighbors=params["n_neighbors"],
                metric=params["metric"],
                algorithm=params["algorithm"],
                leaf_size=params["leaf_size"]
            )
            
            # ìƒí’ˆ íŠ¹ì„±ì„ TF-IDFë¡œ ë²¡í„°í™”
            tfidf = TfidfVectorizer(max_features=1000, stop_words=None)
            product_text = product_features['main_category'].fillna('unknown').astype(str)
            product_vectors = tfidf.fit_transform(product_text)
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(product_vectors)
            print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # í•™ìŠµ ì‹œê°„ ê¸°ë¡
        training_time = time.time() - start_time
        
        # MLflowì— ë©”íŠ¸ë¦­ ê¸°ë¡
        mlflow.log_metric("training_time_seconds", training_time)
        mlflow.log_metric("n_users", len(user_product_matrix))
        mlflow.log_metric("n_products", len(product_features))
        mlflow.log_metric("n_neighbors", params["n_neighbors"])
        
        # ëª¨ë¸ ì €ì¥
        model_path = GUEST_EXPORT_DIR / "guest_model.pkl"
        tfidf_path = GUEST_EXPORT_DIR / "tfidf_vectorizer.pkl"
        product_features_path = GUEST_EXPORT_DIR / "product_features.pkl"
        user_product_matrix_path = GUEST_EXPORT_DIR / "user_product_matrix.pkl"
        
        joblib.dump(model, model_path)
        joblib.dump(tfidf, tfidf_path)
        joblib.dump(product_features, product_features_path)
        joblib.dump(user_product_matrix, user_product_matrix_path)
        
        # MLflowì— ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ê¸°ë¡
        mlflow.log_artifact(str(model_path), "models")
        mlflow.log_artifact(str(tfidf_path), "models")
        mlflow.log_artifact(str(product_features_path), "models")
        mlflow.log_artifact(str(user_product_matrix_path), "models")
        
        # ëª¨ë¸ ì •ë³´ë¥¼ MLflowì— ë“±ë¡ (API ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°)
        # mlflow.sklearn.log_model(model, "guest_recommendation_model")
        
        # ğŸ¯ ëª¨ë¸ ì•„ì¹´ì´ë¸Œ ë° ì„œë¹™ìš© ë³µì‚¬
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ì´ì „ ëª¨ë¸ì„ ì•„ì¹´ì´ë¸Œë¡œ ì´ë™
        if (GUEST_LATEST_DIR / "guest_model.pkl").exists():
            archive_dir = GUEST_ARCHIVE_DIR / f"guest_models_{timestamp}"
            archive_dir.mkdir(parents=True, exist_ok=True)
            
            for file_path in GUEST_LATEST_DIR.glob("*"):
                if file_path.is_file():
                    shutil.copy2(file_path, archive_dir / file_path.name)
            print(f"ğŸ“¦ ì´ì „ ëª¨ë¸ì„ ì•„ì¹´ì´ë¸Œì— ì €ì¥: {archive_dir}")
        
        # 2. ì„œë¹™ìš© ë””ë ‰í† ë¦¬ì— ë³µì‚¬
        for file_path in GUEST_LATEST_DIR.glob("*"):
            if file_path.is_file():
                shutil.copy2(file_path, GUEST_SERVING_DIR / file_path.name)
        print(f"ğŸš€ ì„œë¹™ìš© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {GUEST_SERVING_DIR}")
        
        print(f"âœ… ê²ŒìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print(f"   - í•™ìŠµ ì‹œê°„: {training_time:.2f}ì´ˆ")
        print(f"   - ì‚¬ìš©ì ìˆ˜: {len(user_product_matrix)}")
        print(f"   - ìƒí’ˆ ìˆ˜: {len(product_features)}")
        print(f"   - í•™ìŠµ ëª¨ë“œ: {'ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸' if existing_model_path.exists() else 'ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ'}")
        print(f"   - ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {GUEST_EXPORT_DIR}")
        print(f"   - ì•„ì¹´ì´ë¸Œ ìœ„ì¹˜: {GUEST_ARCHIVE_DIR}")
        print(f"   - ì„œë¹™ ìœ„ì¹˜: {GUEST_SERVING_DIR}")
        
        return model, tfidf, product_features, user_product_matrix

# ===============================
# 3) ë©”ì¸ ì‹¤í–‰
# ===============================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("=== ê²ŒìŠ¤íŠ¸ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ (Airflow DBì—ì„œ)
        print("ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘... (ìµœê·¼ ì¼ì£¼ì¼)")
        df_training = load_dataframe()
        
        if df_training.empty:
            print("âš ï¸ Airflow DBì—ì„œ ìµœê·¼ ì¼ì£¼ì¼ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_training)}ê±´ (ìµœê·¼ ì¼ì£¼ì¼)")
        
        # ëª¨ë¸ í•™ìŠµ
        model, tfidf, product_features, user_product_matrix = train_guest_model(
            df_training, 
            experiment_name="guest_recommendation"
        )
        
        # SSH ì—°ê²° í…ŒìŠ¤íŠ¸ (Jetson ì „ì†¡ ì¤€ë¹„)
        if os.getenv("JETSON_PUSH", "0") == "1":
            print("Jetson ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                ssh_test()
                print("âœ… Jetson ì—°ê²° ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Jetson ì—°ê²° ì‹¤íŒ¨: {e}")
        
        print("=== ê²ŒìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ===")
        
    except Exception as e:
        print(f"âŒ ê²ŒìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()
